{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip The Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflite-model-maker\n",
      "  Using cached tflite_model_maker-0.4.2-py3-none-any.whl (577 kB)\n",
      "Collecting tensorflowjs<3.19.0,>=2.4.0\n",
      "  Downloading tensorflowjs-3.18.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m306.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /home/erklarungsnot/.local/lib/python3.10/site-packages (from tflite-model-maker) (1.16.0)\n",
      "Collecting tflite-model-maker\n",
      "  Using cached tflite_model_maker-0.4.1-py3-none-any.whl (642 kB)\n",
      "  Using cached tflite_model_maker-0.4.0-py3-none-any.whl (642 kB)\n",
      "  Using cached tflite_model_maker-0.3.4-py3-none-any.whl (616 kB)\n",
      "Collecting matplotlib<3.5.0,>=3.0.3\n",
      "  Using cached matplotlib-3.4.3.tar.gz (37.9 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tf-models-official==2.3.0\n",
      "  Using cached tf_models_official-2.3.0-py2.py3-none-any.whl (840 kB)\n",
      "Collecting tensorflowjs>=2.4.0\n",
      "  Using cached tensorflowjs-4.4.0-py3-none-any.whl (85 kB)\n",
      "Collecting neural-structured-learning>=1.3.1\n",
      "  Using cached neural_structured_learning-1.4.0-py2.py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: tensorflow>=2.6.0 in /home/erklarungsnot/miniconda3/lib/python3.10/site-packages (from tflite-model-maker) (2.11.0)\n",
      "Collecting tensorflow-datasets>=2.1.0\n",
      "  Using cached tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "Collecting tensorflow-model-optimization>=0.5\n",
      "  Using cached tensorflow_model_optimization-0.7.4-py2.py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: lxml>=4.6.1 in /home/erklarungsnot/miniconda3/lib/python3.10/site-packages (from tflite-model-maker) (4.9.2)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Collecting tflite-model-maker\n",
      "  Using cached tflite_model_maker-0.3.3-py3-none-any.whl (616 kB)\n",
      "  Using cached tflite_model_maker-0.3.2-py3-none-any.whl (591 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/erklarungsnot/.local/lib/python3.10/site-packages (from tflite-model-maker) (3.6.0)\n",
      "  Using cached tflite_model_maker-0.3.1-py3-none-any.whl (590 kB)\n",
      "  Using cached tflite_model_maker-0.3.0-py3-none-any.whl (567 kB)\n",
      "  Using cached tflite_model_maker-0.2.5-py3-none-any.whl (499 kB)\n",
      "Collecting tensorflow-hub<0.10>=0.8.0\n",
      "  Using cached tensorflow_hub-0.9.0-py2.py3-none-any.whl (103 kB)\n",
      "Collecting absl-py<0.11>=0.10.0\n",
      "  Using cached absl_py-0.10.0-py3-none-any.whl (127 kB)\n",
      "Collecting librosa>=0.5\n",
      "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "Requirement already satisfied: pillow>=7.0.0 in /home/erklarungsnot/miniconda3/lib/python3.10/site-packages (from tflite-model-maker) (9.4.0)\n",
      "Collecting tensorflow-addons>=0.11.2\n",
      "  Using cached tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
      "Collecting fire>=0.3.1\n",
      "  Using cached fire-0.5.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tflite-model-maker\n",
      "  Using cached tflite_model_maker-0.2.4-py3-none-any.whl (190 kB)\n",
      "Collecting flatbuffers==1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Using cached tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting sentencepiece>=0.1.91\n",
      "  Using cached sentencepiece-0.1.98-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/erklarungsnot/.local/lib/python3.10/site-packages (from tflite-model-maker) (1.23.3)\n",
      "Collecting tflite-model-maker\n",
      "  Using cached tflite_model_maker-0.2.3-py3-none-any.whl (114 kB)\n",
      "  Using cached tflite_model_maker-0.2.2-py3-none-any.whl (103 kB)\n",
      "  Using cached tflite_model_maker-0.2.1-py3-none-any.whl (102 kB)\n",
      "  Using cached tflite_model_maker-0.2.0-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: absl-py in /home/erklarungsnot/.local/lib/python3.10/site-packages (from tflite-model-maker) (1.2.0)\n",
      "Collecting tf-models-official\n",
      "  Using cached tf_models_official-2.12.0-py2.py3-none-any.whl (2.6 MB)\n",
      "Collecting tflite-model-maker\n",
      "  Using cached tflite_model_maker-0.1.2-py3-none-any.whl (104 kB)\n",
      "Collecting tf-models-nightly\n",
      "  Using cached tf_models_nightly-2.12.0.dev20230426-py2.py3-none-any.whl (2.6 MB)\n",
      "Collecting tf-nightly\n"
     ]
    }
   ],
   "source": [
    "!./unzip.sh UCI_HAR_Dataset.zip 2>&1 > /dev/null\n",
    "!pip install tflite-model-maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape) + [nb_classes])\n",
    "\n",
    "\n",
    "def load_y(subset):\n",
    "    # Get the path\n",
    "    path = f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/y_{subset}.txt\"\n",
    "\n",
    "    # Read the file\n",
    "    y = np.loadtxt(path, delimiter=\",\", dtype=int)\n",
    "\n",
    "    # # One-hot encode labels\n",
    "    one_hot_labels = get_one_hot(y - 1, len(np.unique(y)))\n",
    "    if subset == \"train\":\n",
    "        assert one_hot_labels.shape == (\n",
    "            7352,\n",
    "            6,\n",
    "        ), f\"Wrong dimensions: {one_hot_labels.shape} should be (7352, 6)\"\n",
    "    if subset == \"test\":\n",
    "        assert one_hot_labels.shape == (\n",
    "            2947,\n",
    "            6,\n",
    "        ), f\"Wrong dimensions: {one_hot_labels.shape} should be (2947, 6)\"\n",
    "    assert (\n",
    "        y[0] - 1 == np.where(one_hot_labels[0] == np.max(one_hot_labels[0]))[0][0]\n",
    "    ), f\"Value mismatch {np.max(one_hot_labels[0])[0][0]} vs {y[13] - 1}\"\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def build_data(subset):\n",
    "    if subset not in [\"train\", \"val\", \"test\"]:\n",
    "        raise Exception(f\"Invalid subset: {subset}\")\n",
    "\n",
    "    folder_path = f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/\"\n",
    "\n",
    "    # Get all signal files in folder\n",
    "    signal_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    # print(signal_files)\n",
    "\n",
    "    assert len(signal_files) == 9, f\"No signal files found in {folder_path}\"\n",
    "    np.loadtxt(signal_files[0]).shape\n",
    "    # print(f\"{signal_shape}\")\n",
    "\n",
    "    # Determine signal order based on file names\n",
    "    signal_order = [\n",
    "        \"body_acc_x_\",\n",
    "        \"body_acc_y_\",\n",
    "        \"body_acc_z_\",\n",
    "        \"body_gyro_x_\",\n",
    "        \"body_gyro_y_\",\n",
    "        \"body_gyro_z_\",\n",
    "        \"total_acc_x_\",\n",
    "        \"total_acc_y_\",\n",
    "        \"total_acc_z_\",\n",
    "    ]\n",
    "\n",
    "    # file_prefix = \"UCI_HAR_Dataset/UCI_HAR_Dataset/train/Inertial Signals/\"\n",
    "    # file_suffix = \".txt\"\n",
    "    signal_files = [\n",
    "        f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/{x}{subset}.txt\"\n",
    "        for x in signal_order\n",
    "    ]\n",
    "\n",
    "    # Load signal data from each file and append to signals_data list\n",
    "    signals_data = [np.loadtxt(x) for x in signal_files]\n",
    "\n",
    "    # Transpose signal data array so that shape is (number of samples, number of timesteps, number of signals)\n",
    "    signals_data = np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "    # Verify final shape of combined data\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    if subset == \"train\":\n",
    "        assert signals_data.shape == (7352, 128, len(signal_files))\n",
    "    else:\n",
    "        assert signals_data.shape == (2947, 128, len(signal_files))\n",
    "    return signals_data\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    return build_data(\"train\"), load_y(\"train\"), build_data(\"test\"), load_y(\"test\")\n",
    "\n",
    "\n",
    "# Loading the train and test data\n",
    "X_train, y_train, X_test, y_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = X_train[0]\n",
    "first_timestep = first_sample[0]\n",
    "assert len(first_sample) == 128\n",
    "assert first_timestep[0] == 1.8085150e-004, print(first_timestep[0])\n",
    "assert first_timestep[1] == 1.0766810e-002, print(first_timestep[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "assert X_train.shape == (7352, 128, 9), print(\"Expected shape: (7352, 128, 9) get\", X_train.shape)\n",
    "assert X_test.shape == (2947, 128, 9), print(\"Expected: (2947, 128, 9) get\", X_test.shape)\n",
    "assert y_train.shape == (7352, 6), print(\"Expected: (7352, 6) get\", y_train.shape)\n",
    "assert y_test.shape == (2947, 6), print(\"Expected: (2947, 6) get\", y_test.shape)\n",
    "assert len(X_train[0][0]) == 9, print(\"Signals numbers not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create the \"assets\" folder if it does not exist\n",
    "if not os.path.exists(\"assets\"):\n",
    "    os.mkdir(\"assets\")\n",
    "\n",
    "# Create the \"assets/data\" folder if it does not exist\n",
    "data_folder = os.path.join(\"assets\", \"data\")\n",
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "\n",
    "def save_data_to_pickle_shards(data, data_name, data_folder):\n",
    "    # Check if the data already exists\n",
    "    filename = os.path.join(data_folder, f\"{data_name}_0.pickle\")\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{data_name} already exists in {data_folder}. Skipping data saving.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(os.path.join(data_folder)):\n",
    "        os.makedirs(os.path.join(data_folder))\n",
    "\n",
    "    # Serialize your data\n",
    "    serialized_data = pickle.dumps(data)\n",
    "\n",
    "    # Split the serialized data into smaller chunks\n",
    "    chunk_size = 50 * 1024 * 1024  # 50 megabytes\n",
    "    chunks = [\n",
    "        serialized_data[i : i + chunk_size]\n",
    "        for i in range(0, len(serialized_data), chunk_size)\n",
    "    ]\n",
    "\n",
    "    # Save each chunk to a file in the \"asset/data\" folder\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        filename = os.path.join(data_folder, f\"{data_name}_{i}.pickle\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(chunk)\n",
    "\n",
    "\n",
    "save_data_to_pickle_shards(X_train, \"X_train\", data_folder)\n",
    "save_data_to_pickle_shards(y_train, \"y_train\", data_folder)\n",
    "save_data_to_pickle_shards(X_test, \"X_test\", data_folder)\n",
    "save_data_to_pickle_shards(y_test, \"y_test\", data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_data_from_pickle_shards(data_name, data_folder):\n",
    "    # Find all pickle files that match the data name\n",
    "    files = sorted(\n",
    "        [\n",
    "            os.path.join(data_folder, f)\n",
    "            for f in os.listdir(data_folder)\n",
    "            if f.startswith(data_name)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load the data from each file\n",
    "    data = b\"\"\n",
    "    for filename in files:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data += f.read()\n",
    "\n",
    "    # Deserialize the data\n",
    "    return pickle.loads(data)\n",
    "\n",
    "# Load the data from the pickle shards\n",
    "loaded_X_train = load_data_from_pickle_shards(\"X_train\", data_folder)\n",
    "loaded_y_train = load_data_from_pickle_shards(\"y_train\", data_folder)\n",
    "loaded_X_test = load_data_from_pickle_shards(\"X_test\", data_folder)\n",
    "loaded_y_test = load_data_from_pickle_shards(\"y_test\", data_folder)\n",
    "\n",
    "# Check if the loaded data matches the original data\n",
    "assert loaded_X_train.shape == X_train.shape\n",
    "assert loaded_y_train.shape == y_train.shape\n",
    "assert loaded_X_test.shape == X_test.shape\n",
    "assert loaded_y_test.shape == y_test.shape\n",
    "\n",
    "assert (loaded_X_train == X_train).all()\n",
    "assert (loaded_y_train == y_train).all()\n",
    "assert (loaded_X_test == X_test).all()\n",
    "assert (loaded_y_test == y_test).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: 128\n",
      "Input dimention: 9\n",
      "Total samples: 7352\n"
     ]
    }
   ],
   "source": [
    "# function to count the number of classes\n",
    "def count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))\n",
    "\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = count_classes(y_train)\n",
    "\n",
    "# Initializing parameters\n",
    "n_epochs = 30\n",
    "n_batch = 16\n",
    "\n",
    "# Bias regularizer value - we will use elasticnet\n",
    "regularizer = L1L2(0.01, 0.01)\n",
    "\n",
    "print(f\"Timesteps: {timesteps}\")\n",
    "print(f\"Input dimention: {input_dim}\")\n",
    "print(f\"Total samples: {len(X_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 04:20:02.994140: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-26 04:20:02.994233: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 04:20:02.994299: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 04:20:02.994514: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-04-26 04:20:03.117029: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-04-26 04:20:03.123220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128, 64)           18944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128, 64)          256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128, 64)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 48)                21696     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 48)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 294       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,190\n",
      "Trainable params: 41,062\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model execution\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64,\n",
    "        input_shape=(timesteps, input_dim),\n",
    "        return_sequences=True,\n",
    "        bias_regularizer=regularizer,\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.50))\n",
    "model.add(LSTM(48))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(n_classes, activation=\"sigmoid\"))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "460/460 [==============================] - 25s 50ms/step - loss: 1.6479 - accuracy: 0.7326 - val_loss: 1.0388 - val_accuracy: 0.8409\n",
      "Epoch 2/30\n",
      "460/460 [==============================] - 22s 49ms/step - loss: 0.7173 - accuracy: 0.9034 - val_loss: 0.5317 - val_accuracy: 0.8979\n",
      "Epoch 3/30\n",
      "460/460 [==============================] - 22s 48ms/step - loss: 0.3557 - accuracy: 0.9238 - val_loss: 0.2667 - val_accuracy: 0.9141\n",
      "Epoch 4/30\n",
      "460/460 [==============================] - 23s 49ms/step - loss: 0.2278 - accuracy: 0.9251 - val_loss: 0.3492 - val_accuracy: 0.8768\n",
      "Epoch 5/30\n",
      "460/460 [==============================] - 23s 49ms/step - loss: 0.3533 - accuracy: 0.8837 - val_loss: 0.3417 - val_accuracy: 0.8880\n",
      "Epoch 6/30\n",
      "460/460 [==============================] - 23s 50ms/step - loss: 0.2079 - accuracy: 0.9301 - val_loss: 0.2434 - val_accuracy: 0.9063\n",
      "Epoch 7/30\n",
      "460/460 [==============================] - 23s 50ms/step - loss: 0.1731 - accuracy: 0.9327 - val_loss: 0.2302 - val_accuracy: 0.9145\n",
      "Epoch 8/30\n",
      "460/460 [==============================] - 23s 51ms/step - loss: 0.1567 - accuracy: 0.9385 - val_loss: 0.2705 - val_accuracy: 0.8999\n",
      "Epoch 9/30\n",
      "460/460 [==============================] - 24s 53ms/step - loss: 0.1606 - accuracy: 0.9400 - val_loss: 0.2580 - val_accuracy: 0.9165\n",
      "Epoch 10/30\n",
      "460/460 [==============================] - 27s 58ms/step - loss: 0.3097 - accuracy: 0.9023 - val_loss: 0.5741 - val_accuracy: 0.7872\n",
      "Epoch 11/30\n",
      "460/460 [==============================] - 25s 54ms/step - loss: 0.1801 - accuracy: 0.9348 - val_loss: 0.2730 - val_accuracy: 0.9094\n",
      "Epoch 12/30\n",
      "460/460 [==============================] - 23s 51ms/step - loss: 0.1561 - accuracy: 0.9422 - val_loss: 0.2344 - val_accuracy: 0.9165\n",
      "Epoch 13/30\n",
      "460/460 [==============================] - 24s 52ms/step - loss: 0.1465 - accuracy: 0.9436 - val_loss: 0.2833 - val_accuracy: 0.9141\n",
      "Epoch 14/30\n",
      "460/460 [==============================] - 23s 51ms/step - loss: 0.1516 - accuracy: 0.9372 - val_loss: 0.2428 - val_accuracy: 0.9128\n",
      "Epoch 15/30\n",
      "460/460 [==============================] - 25s 54ms/step - loss: 0.1386 - accuracy: 0.9444 - val_loss: 0.2544 - val_accuracy: 0.9209\n",
      "Epoch 16/30\n",
      "460/460 [==============================] - 24s 52ms/step - loss: 0.1399 - accuracy: 0.9436 - val_loss: 0.2363 - val_accuracy: 0.9237\n",
      "Epoch 17/30\n",
      "460/460 [==============================] - 27s 59ms/step - loss: 0.1750 - accuracy: 0.9354 - val_loss: 0.2363 - val_accuracy: 0.9169\n",
      "Epoch 18/30\n",
      "460/460 [==============================] - 27s 59ms/step - loss: 0.1432 - accuracy: 0.9442 - val_loss: 0.2479 - val_accuracy: 0.9125\n",
      "Epoch 19/30\n",
      "460/460 [==============================] - 27s 59ms/step - loss: 0.1437 - accuracy: 0.9434 - val_loss: 0.2620 - val_accuracy: 0.9104\n",
      "Epoch 20/30\n",
      "460/460 [==============================] - 25s 55ms/step - loss: 0.1382 - accuracy: 0.9441 - val_loss: 0.3090 - val_accuracy: 0.8785\n",
      "Epoch 21/30\n",
      "460/460 [==============================] - 27s 59ms/step - loss: 0.1325 - accuracy: 0.9471 - val_loss: 0.2582 - val_accuracy: 0.9158\n",
      "Epoch 22/30\n",
      "460/460 [==============================] - 29s 64ms/step - loss: 0.1256 - accuracy: 0.9470 - val_loss: 0.2975 - val_accuracy: 0.9155\n",
      "Epoch 23/30\n",
      "460/460 [==============================] - 29s 62ms/step - loss: 0.1412 - accuracy: 0.9429 - val_loss: 0.3941 - val_accuracy: 0.8999\n",
      "Epoch 24/30\n",
      "460/460 [==============================] - 34s 73ms/step - loss: 0.1481 - accuracy: 0.9418 - val_loss: 0.2469 - val_accuracy: 0.9050\n",
      "Epoch 25/30\n",
      "460/460 [==============================] - 32s 69ms/step - loss: 0.1373 - accuracy: 0.9457 - val_loss: 0.3546 - val_accuracy: 0.8880\n",
      "Epoch 26/30\n",
      "460/460 [==============================] - 30s 65ms/step - loss: 0.1527 - accuracy: 0.9436 - val_loss: 0.3576 - val_accuracy: 0.9077\n",
      "Epoch 27/30\n",
      "460/460 [==============================] - 25s 54ms/step - loss: 0.1321 - accuracy: 0.9441 - val_loss: 0.2811 - val_accuracy: 0.9172\n",
      "Epoch 28/30\n",
      "460/460 [==============================] - 25s 55ms/step - loss: 0.1431 - accuracy: 0.9378 - val_loss: 0.2693 - val_accuracy: 0.9114\n",
      "Epoch 29/30\n",
      "323/460 [====================>.........] - ETA: 1:03 - loss: 0.1550 - accuracy: 0.9412"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Training the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=n_batch,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=n_epochs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model(s) for development purposes\n",
    "import tensorflow as tf\n",
    "\n",
    "# create a TFLiteConverter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# set the target ops to TFLITE_BUILTINS and SELECT_TF_OPS\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS,\n",
    "]\n",
    "\n",
    "# disable lowering tensor list operations\n",
    "converter._experimental_lower_tensor_list_ops = False\n",
    "\n",
    "# convert the model to TFLite format\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save the TFLite model to a file\n",
    "with open(\"./assets/model_lstm.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d37fff4659cf8a883ce3d6c1246076e30e33dc297d4df960d23d9670e4eb60f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
