{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip The Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./unzip.sh UCI_HAR_Dataset.zip 2>&1 > /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def load_y(subset):\n",
    "    # Get the path\n",
    "    path = f'UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "\n",
    "    # Read the file\n",
    "    y = np.loadtxt(path, delimiter=',', dtype=int)\n",
    "\n",
    "    # # One-hot encode labels\n",
    "    one_hot_labels = get_one_hot(y - 1 , len(np.unique(y)))\n",
    "    if subset == 'train':\n",
    "        assert one_hot_labels.shape == (7352, 6), f\"Wrong dimensions: {one_hot_labels.shape} should be (7352, 6)\"\n",
    "    if subset == 'test':\n",
    "        assert one_hot_labels.shape == (2947, 6), f\"Wrong dimensions: {one_hot_labels.shape} should be (2947, 6)\"\n",
    "    assert y[0] - 1 == np.where(one_hot_labels[0] == np.max(one_hot_labels[0]))[0][0], f\"Value mismatch {np.max(one_hot_labels[0])[0][0]} vs {y[13] - 1}\"\n",
    "    return one_hot_labels\n",
    "\n",
    "def build_data(subset):\n",
    "    if subset not in ['train', 'val', 'test']:\n",
    "        raise Exception(f\"Invalid subset: {subset}\")\n",
    "\n",
    "    folder_path = f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/\"\n",
    "\n",
    "    # Get all signal files in folder\n",
    "    signal_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    # print(signal_files)\n",
    "\n",
    "    assert len(signal_files) == 9, f\"No signal files found in {folder_path}\"\n",
    "    signal_shape = np.loadtxt(signal_files[0]).shape\n",
    "    # print(f\"{signal_shape}\")\n",
    "\n",
    "    # Determine signal order based on file names\n",
    "    signal_order = [\n",
    "        \"body_acc_x_\",\n",
    "        \"body_acc_y_\",\n",
    "        \"body_acc_z_\",\n",
    "        \"body_gyro_x_\",\n",
    "        \"body_gyro_y_\",\n",
    "        \"body_gyro_z_\",\n",
    "        \"total_acc_x_\",\n",
    "        \"total_acc_y_\",\n",
    "        \"total_acc_z_\",\n",
    "        ]\n",
    "\n",
    "    # file_prefix = \"UCI_HAR_Dataset/UCI_HAR_Dataset/train/Inertial Signals/\"\n",
    "    # file_suffix = \".txt\"\n",
    "    signal_files = [f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/{x}{subset}.txt\" for x in signal_order]\n",
    "\n",
    "    # Load signal data from each file and append to signals_data list\n",
    "    signals_data = [np.loadtxt(x) for x in signal_files]\n",
    "\n",
    "    # Transpose signal data array so that shape is (number of samples, number of timesteps, number of signals)\n",
    "    signals_data = np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "    # Verify final shape of combined data\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    if subset == 'train':\n",
    "        assert signals_data.shape == (7352, 128, len(signal_files))\n",
    "    else:\n",
    "        assert signals_data.shape == (2947, 128, len(signal_files))\n",
    "    return signals_data\n",
    "\n",
    "def load_data():\n",
    "    return build_data('train'), load_y('train'), build_data('test'), load_y('test')\n",
    "\n",
    "# Loading the train and test data\n",
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = X_train[0]\n",
    "first_timestep = first_sample[0]\n",
    "assert len(first_sample) == 128\n",
    "assert first_timestep[0] == 1.8085150e-004, print(first_timestep[0])\n",
    "assert first_timestep[1] == 1.0766810e-002, print(first_timestep[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: 128\n",
      "Input dimention: 9\n",
      "Total samples: 7352\n"
     ]
    }
   ],
   "source": [
    "#function to count the number of classes\n",
    "def count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = count_classes(y_train)\n",
    "\n",
    "# Initializing parameters\n",
    "n_epochs = 30\n",
    "n_batch = 16\n",
    "\n",
    "# Bias regularizer value - we will use elasticnet\n",
    "regularizer = L1L2(0.01, 0.01)\n",
    "\n",
    "print(f\"Timesteps: {timesteps}\")\n",
    "print(f\"Input dimention: {input_dim}\")\n",
    "print(f\"Total samples: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.layers.recurrent.LSTM object at 0x7f111d2dab70>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f33352b82f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.15/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    458\u001b[0m             raise TypeError('The added layer must be '\n\u001b[1;32m    459\u001b[0m                             \u001b[0;34m'an instance of class Layer. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                             'Found: ' + str(layer))\n\u001b[0m\u001b[1;32m    461\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# First layer in model: check that it is an input layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.layers.recurrent.LSTM object at 0x7f111d2dab70>"
     ]
    }
   ],
   "source": [
    "# Model execution\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=regularizer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.50))\n",
    "model.add(LSTM(48))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767f65abfc225a93d300366d933b501344c7257b645c776f0a53861572b8c79f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
