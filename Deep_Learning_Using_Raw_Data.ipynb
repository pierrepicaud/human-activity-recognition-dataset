{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip The Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./unzip.sh UCI_HAR_Dataset.zip 2>&1 > /dev/null\n",
    "!pip install --no-dependencies tflite-model-maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\"calling BaseResourceVariable.__init__.*constraint is deprecated\"\n",
    ")\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape) + [nb_classes])\n",
    "\n",
    "\n",
    "def load_y(subset):\n",
    "    # Get the path\n",
    "    path = f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/y_{subset}.txt\"\n",
    "\n",
    "    # Read the file\n",
    "    y = np.loadtxt(path, delimiter=\",\", dtype=int)\n",
    "\n",
    "    # # One-hot encode labels\n",
    "    one_hot_labels = get_one_hot(y - 1, len(np.unique(y)))\n",
    "    if subset == \"train\":\n",
    "        assert one_hot_labels.shape == (\n",
    "            7352,\n",
    "            6,\n",
    "        ), f\"Wrong dimensions: {one_hot_labels.shape} should be (7352, 6)\"\n",
    "    if subset == \"test\":\n",
    "        assert one_hot_labels.shape == (\n",
    "            2947,\n",
    "            6,\n",
    "        ), f\"Wrong dimensions: {one_hot_labels.shape} should be (2947, 6)\"\n",
    "    assert (\n",
    "        y[0] - 1 == np.where(one_hot_labels[0] == np.max(one_hot_labels[0]))[0][0]\n",
    "    ), f\"Value mismatch {np.max(one_hot_labels[0])[0][0]} vs {y[13] - 1}\"\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def build_data(subset):\n",
    "    if subset not in [\"train\", \"val\", \"test\"]:\n",
    "        raise Exception(f\"Invalid subset: {subset}\")\n",
    "\n",
    "    folder_path = f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/\"\n",
    "\n",
    "    # Get all signal files in folder\n",
    "    signal_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    # print(signal_files)\n",
    "\n",
    "    assert len(signal_files) == 9, f\"No signal files found in {folder_path}\"\n",
    "    np.loadtxt(signal_files[0]).shape\n",
    "    # print(f\"{signal_shape}\")\n",
    "\n",
    "    # Determine signal order based on file names\n",
    "    signal_order = [\n",
    "        \"body_acc_x_\",\n",
    "        \"body_acc_y_\",\n",
    "        \"body_acc_z_\",\n",
    "        \"body_gyro_x_\",\n",
    "        \"body_gyro_y_\",\n",
    "        \"body_gyro_z_\",\n",
    "        \"total_acc_x_\",\n",
    "        \"total_acc_y_\",\n",
    "        \"total_acc_z_\",\n",
    "    ]\n",
    "\n",
    "    # file_prefix = \"UCI_HAR_Dataset/UCI_HAR_Dataset/train/Inertial Signals/\"\n",
    "    # file_suffix = \".txt\"\n",
    "    signal_files = [\n",
    "        f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/{x}{subset}.txt\"\n",
    "        for x in signal_order\n",
    "    ]\n",
    "\n",
    "    # Load signal data from each file and append to signals_data list\n",
    "    signals_data = [np.loadtxt(x) for x in signal_files]\n",
    "\n",
    "    # Transpose signal data array so that shape is (number of samples, number of timesteps, number of signals)\n",
    "    signals_data = np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "    # Verify final shape of combined data\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    if subset == \"train\":\n",
    "        assert signals_data.shape == (7352, 128, len(signal_files))\n",
    "    else:\n",
    "        assert signals_data.shape == (2947, 128, len(signal_files))\n",
    "    return signals_data\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    return build_data(\"train\"), load_y(\"train\"), build_data(\"test\"), load_y(\"test\")\n",
    "\n",
    "\n",
    "# Loading the train and test data\n",
    "X_train, y_train, X_test, y_test = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = X_train[0]\n",
    "first_timestep = first_sample[0]\n",
    "assert len(first_sample) == 128\n",
    "assert first_timestep[0] == 1.8085150e-004, print(first_timestep[0])\n",
    "assert first_timestep[1] == 1.0766810e-002, print(first_timestep[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "assert X_train.shape == (7352, 128, 9), print(\n",
    "    \"Expected shape: (7352, 128, 9) get\", X_train.shape\n",
    ")\n",
    "assert X_test.shape == (2947, 128, 9), print(\n",
    "    \"Expected: (2947, 128, 9) get\", X_test.shape\n",
    ")\n",
    "assert y_train.shape == (7352, 6), print(\"Expected: (7352, 6) get\", y_train.shape)\n",
    "assert y_test.shape == (2947, 6), print(\"Expected: (2947, 6) get\", y_test.shape)\n",
    "assert len(X_train[0][0]) == 9, print(\"Signals numbers not match\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create the \"assets\" folder if it does not exist\n",
    "if not os.path.exists(\"assets\"):\n",
    "    os.mkdir(\"assets\")\n",
    "\n",
    "# Create the \"assets/data\" folder if it does not exist\n",
    "data_folder = os.path.join(\"assets\", \"data\")\n",
    "if not os.path.exists(data_folder):\n",
    "    os.mkdir(data_folder)\n",
    "\n",
    "\n",
    "def save_data_to_pickle_shards(data, data_name, data_folder):\n",
    "    # Check if the data already exists\n",
    "    filename = os.path.join(data_folder, f\"{data_name}_0.pickle\")\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{data_name} already exists in {data_folder}. Skipping data saving.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(os.path.join(data_folder)):\n",
    "        os.makedirs(os.path.join(data_folder))\n",
    "\n",
    "    # Serialize your data\n",
    "    serialized_data = pickle.dumps(data)\n",
    "\n",
    "    # Split the serialized data into smaller chunks\n",
    "    chunk_size = 50 * 1024 * 1024  # 50 megabytes\n",
    "    chunks = [\n",
    "        serialized_data[i : i + chunk_size]\n",
    "        for i in range(0, len(serialized_data), chunk_size)\n",
    "    ]\n",
    "\n",
    "    # Save each chunk to a file in the \"asset/data\" folder\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        filename = os.path.join(data_folder, f\"{data_name}_{i}.pickle\")\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_pickle_shards(X_train, \"X_train\", data_folder)\n",
    "save_data_to_pickle_shards(y_train, \"y_train\", data_folder)\n",
    "save_data_to_pickle_shards(X_test, \"X_test\", data_folder)\n",
    "save_data_to_pickle_shards(y_test, \"y_test\", data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_data_from_pickle_shards(data_name, data_folder):\n",
    "    # Find all pickle files that match the data name\n",
    "    files = sorted(\n",
    "        [\n",
    "            os.path.join(data_folder, f)\n",
    "            for f in os.listdir(data_folder)\n",
    "            if f.startswith(data_name)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load the data from each file\n",
    "    data = b\"\"\n",
    "    for filename in files:\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data += f.read()\n",
    "\n",
    "    # Deserialize the data\n",
    "    return pickle.loads(data)\n",
    "\n",
    "\n",
    "# Load the data from the pickle shards\n",
    "loaded_X_train = load_data_from_pickle_shards(\"X_train\", data_folder)\n",
    "loaded_y_train = load_data_from_pickle_shards(\"y_train\", data_folder)\n",
    "loaded_X_test = load_data_from_pickle_shards(\"X_test\", data_folder)\n",
    "loaded_y_test = load_data_from_pickle_shards(\"y_test\", data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the loaded data matches the original data\n",
    "assert loaded_X_train.shape == X_train.shape\n",
    "assert loaded_y_train.shape == y_train.shape\n",
    "assert loaded_X_test.shape == X_test.shape\n",
    "assert loaded_y_test.shape == y_test.shape\n",
    "\n",
    "assert (loaded_X_train == X_train).all()\n",
    "assert (loaded_y_train == y_train).all()\n",
    "assert (loaded_X_test == X_test).all()\n",
    "assert (loaded_y_test == y_test).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count the number of classes\n",
    "def count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))\n",
    "\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = count_classes(y_train)\n",
    "\n",
    "# Initializing parameters\n",
    "n_epochs = 30\n",
    "n_batch = 16\n",
    "\n",
    "# Bias regularizer value - we will use elasticnet\n",
    "regularizer = L1L2(0.01, 0.01)\n",
    "\n",
    "print(f\"Timesteps: {timesteps}\")\n",
    "print(f\"Input dimention: {input_dim}\")\n",
    "print(f\"Total samples: {len(X_train)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model execution\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        64,\n",
    "        input_shape=(timesteps, input_dim),\n",
    "        return_sequences=True,\n",
    "        bias_regularizer=regularizer,\n",
    "    )\n",
    ")\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.50))\n",
    "model.add(LSTM(48))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(n_classes, activation=\"sigmoid\"))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# Training the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=n_batch,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=n_epochs,\n",
    ")\n",
    "model.save(\"assets/model-backup.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying Rebuilt Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "success_thresh = 10\n",
    "passed = 0\n",
    "\n",
    "def compare_array_halves(array1, array2):\n",
    "    n = len(array1) // 2\n",
    "    return np.array_equal(array1[:n], array2[n:])\n",
    "\n",
    "\n",
    "for i in range(len(loaded_X_train)):\n",
    "    for j in range(len(loaded_X_train)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        result = compare_array_halves(loaded_X_train[i], loaded_X_train[j])\n",
    "        if result:\n",
    "            passed += 1\n",
    "            print(\n",
    "                f\"The first half of array{i} is equal to the second half of array{j}.\"\n",
    "            )\n",
    "            break\n",
    "    if passed == success_thresh:\n",
    "        print(f\"Success threshhold passed, stopping check\")\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilding Model From Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"assets/model-backup.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking The Rebuilt Model From Backup File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = X_test[0].reshape((1, 128, 9))\n",
    "\n",
    "# Let's check:\n",
    "np.testing.assert_allclose(\n",
    "    model.predict(test_input), reconstructed_model.predict(test_input)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array(model.predict(test_input)).argmax() + 1 == np.array(y_test[0]).argmax() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be used to reconstruct the model identically.\n",
    "reconstructed_model = keras.models.load_model(\"assets/model-backup.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Keras Model to TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "kernelspec": {
  "display_name": "Python 3.8",
  "name": "py38"
 },
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "767f65abfc225a93d300366d933b501344c7257b645c776f0a53861572b8c79f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
