{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip The Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./unzip.sh UCI_HAR_Dataset.zip 2>&1 > /dev/null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def load_y(subset):\n",
    "    # Get the path\n",
    "    path = f'UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "\n",
    "    # Read the file\n",
    "    y = np.loadtxt(path, delimiter=',', dtype=int)\n",
    "\n",
    "    # # One-hot encode labels\n",
    "    one_hot_labels = get_one_hot(y - 1 , len(np.unique(y)))\n",
    "    if subset == 'train':\n",
    "        assert one_hot_labels.shape == (7352, 6), f\"Wrong dimensions: {one_hot_labels.shape} should be (7352, 6)\"\n",
    "    if subset == 'test':\n",
    "        assert one_hot_labels.shape == (2947, 6), f\"Wrong dimensions: {one_hot_labels.shape} should be (2947, 6)\"\n",
    "    assert y[0] - 1 == np.where(one_hot_labels[0] == np.max(one_hot_labels[0]))[0][0], f\"Value mismatch {np.max(one_hot_labels[0])[0][0]} vs {y[13] - 1}\"\n",
    "    return one_hot_labels\n",
    "\n",
    "def build_data(subset):\n",
    "    if subset not in ['train', 'val', 'test']:\n",
    "        raise Exception(f\"Invalid subset: {subset}\")\n",
    "\n",
    "    folder_path = f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/\"\n",
    "\n",
    "    # Get all signal files in folder\n",
    "    signal_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "    # print(signal_files)\n",
    "\n",
    "    assert len(signal_files) == 9, f\"No signal files found in {folder_path}\"\n",
    "    signal_shape = np.loadtxt(signal_files[0]).shape\n",
    "    # print(f\"{signal_shape}\")\n",
    "\n",
    "    # Determine signal order based on file names\n",
    "    signal_order = [\n",
    "        \"body_acc_x_\",\n",
    "        \"body_acc_y_\",\n",
    "        \"body_acc_z_\",\n",
    "        \"body_gyro_x_\",\n",
    "        \"body_gyro_y_\",\n",
    "        \"body_gyro_z_\",\n",
    "        \"total_acc_x_\",\n",
    "        \"total_acc_y_\",\n",
    "        \"total_acc_z_\",\n",
    "        ]\n",
    "\n",
    "    # file_prefix = \"UCI_HAR_Dataset/UCI_HAR_Dataset/train/Inertial Signals/\"\n",
    "    # file_suffix = \".txt\"\n",
    "    signal_files = [f\"UCI_HAR_Dataset/UCI_HAR_Dataset/{subset}/Inertial Signals/{x}{subset}.txt\" for x in signal_order]\n",
    "\n",
    "    # Load signal data from each file and append to signals_data list\n",
    "    signals_data = [np.loadtxt(x) for x in signal_files]\n",
    "\n",
    "    # Transpose signal data array so that shape is (number of samples, number of timesteps, number of signals)\n",
    "    signals_data = np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "    # Verify final shape of combined data\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    if subset == 'train':\n",
    "        assert signals_data.shape == (7352, 128, len(signal_files))\n",
    "    else:\n",
    "        assert signals_data.shape == (2947, 128, len(signal_files))\n",
    "    return signals_data\n",
    "\n",
    "def load_data():\n",
    "    return build_data('train'), load_y('train'), build_data('test'), load_y('test')\n",
    "\n",
    "# Loading the train and test data\n",
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = X_train[0]\n",
    "first_timestep = first_sample[0]\n",
    "assert len(first_sample) == 128\n",
    "assert first_timestep[0] == 1.8085150e-004, print(first_timestep[0])\n",
    "assert first_timestep[1] == 1.0766810e-002, print(first_timestep[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: 128\n",
      "Input dimention: 9\n",
      "Total samples: 7352\n"
     ]
    }
   ],
   "source": [
    "#function to count the number of classes\n",
    "def count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))\n",
    "\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = count_classes(y_train)\n",
    "\n",
    "# Initializing parameters\n",
    "n_epochs = 30\n",
    "n_batch = 16\n",
    "\n",
    "# Bias regularizer value - we will use elasticnet\n",
    "regularizer = L1L2(0.01, 0.01)\n",
    "\n",
    "print(f\"Timesteps: {timesteps}\")\n",
    "print(f\"Input dimention: {input_dim}\")\n",
    "print(f\"Total samples: {len(X_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/erklarungsnot/.pyenv/versions/3.6.15/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128, 64)           18944     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 64)           256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 48)                21696     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 294       \n",
      "=================================================================\n",
      "Total params: 41,190\n",
      "Trainable params: 41,062\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model execution\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(timesteps, input_dim), return_sequences=True, bias_regularizer=regularizer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.50))\n",
    "model.add(LSTM(48))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/erklarungsnot/.pyenv/versions/3.6.15/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 42s 6ms/sample - loss: 1.8424 - acc: 0.6533 - val_loss: 1.3844 - val_acc: 0.6868\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 43s 6ms/sample - loss: 1.0107 - acc: 0.8124 - val_loss: 1.1238 - val_acc: 0.7248\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 44s 6ms/sample - loss: 0.6296 - acc: 0.8845 - val_loss: 0.5119 - val_acc: 0.8856\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 45s 6ms/sample - loss: 0.4249 - acc: 0.9010 - val_loss: 0.3619 - val_acc: 0.8867\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 44s 6ms/sample - loss: 0.3209 - acc: 0.9110 - val_loss: 0.3506 - val_acc: 0.8785\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 46s 6ms/sample - loss: 0.2395 - acc: 0.9245 - val_loss: 0.2257 - val_acc: 0.9179\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 45s 6ms/sample - loss: 0.2168 - acc: 0.9256 - val_loss: 0.2845 - val_acc: 0.8992\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 43s 6ms/sample - loss: 0.2101 - acc: 0.9295 - val_loss: 0.3419 - val_acc: 0.8867\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 46s 6ms/sample - loss: 0.2146 - acc: 0.9251 - val_loss: 0.3172 - val_acc: 0.9023\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 44s 6ms/sample - loss: 0.1859 - acc: 0.9332 - val_loss: 0.2128 - val_acc: 0.9230\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 54s 7ms/sample - loss: 0.1663 - acc: 0.9380 - val_loss: 0.2644 - val_acc: 0.9121\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 51s 7ms/sample - loss: 0.1901 - acc: 0.9317 - val_loss: 0.2756 - val_acc: 0.9158\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 61s 8ms/sample - loss: 0.1579 - acc: 0.9404 - val_loss: 0.3090 - val_acc: 0.8904\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 63s 9ms/sample - loss: 0.1638 - acc: 0.9391 - val_loss: 0.2329 - val_acc: 0.9158\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 60s 8ms/sample - loss: 0.1465 - acc: 0.9423 - val_loss: 0.2248 - val_acc: 0.9243\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 55s 7ms/sample - loss: 0.2084 - acc: 0.9279 - val_loss: 0.2011 - val_acc: 0.9186\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 48s 7ms/sample - loss: 0.1951 - acc: 0.9344 - val_loss: 0.2159 - val_acc: 0.9104\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 49s 7ms/sample - loss: 0.1829 - acc: 0.9346 - val_loss: 0.1910 - val_acc: 0.9243\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 50s 7ms/sample - loss: 0.1561 - acc: 0.9421 - val_loss: 0.2006 - val_acc: 0.9175\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 57s 8ms/sample - loss: 0.1416 - acc: 0.9467 - val_loss: 0.2081 - val_acc: 0.9114\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 50s 7ms/sample - loss: 0.1374 - acc: 0.9455 - val_loss: 0.2447 - val_acc: 0.9192\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 46s 6ms/sample - loss: 0.1461 - acc: 0.9461 - val_loss: 0.2623 - val_acc: 0.8999\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 47s 6ms/sample - loss: 0.1710 - acc: 0.9372 - val_loss: 0.4217 - val_acc: 0.8867\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 47s 6ms/sample - loss: 0.1556 - acc: 0.9446 - val_loss: 0.2358 - val_acc: 0.9369\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 51s 7ms/sample - loss: 0.1500 - acc: 0.9421 - val_loss: 0.2024 - val_acc: 0.9257\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 47s 6ms/sample - loss: 0.1381 - acc: 0.9441 - val_loss: 0.1871 - val_acc: 0.9291\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 45s 6ms/sample - loss: 0.1269 - acc: 0.9513 - val_loss: 0.2177 - val_acc: 0.9382\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 46s 6ms/sample - loss: 0.1448 - acc: 0.9468 - val_loss: 0.2161 - val_acc: 0.9220\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 65s 9ms/sample - loss: 0.1524 - acc: 0.9388 - val_loss: 0.2534 - val_acc: 0.9111\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 62s 8ms/sample - loss: 0.1914 - acc: 0.9302 - val_loss: 0.2420 - val_acc: 0.9155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f646f2ff7b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, batch_size=n_batch, validation_data=(X_test, y_test), epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'TFLiteConverter' has no attribute 'from_keras_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1d0be70467b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create a TFLiteConverter object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# convert the model to TFLite format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'TFLiteConverter' has no attribute 'from_keras_model'"
     ]
    }
   ],
   "source": [
    "# Save model(s) for development purposes\n",
    "import tensorflow as tf\n",
    "\n",
    "# create a TFLiteConverter object\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "# convert the model to TFLite format\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save the TFLite model to a file\n",
    "with open('./assets/model_lstm.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767f65abfc225a93d300366d933b501344c7257b645c776f0a53861572b8c79f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
